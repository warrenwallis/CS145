{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 1 \n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "The submission deadline for all homeworks are one week from its release date.\n",
    "HW1 is due on **11:59 PM PT, April 19 (Wednesday)**. Please submit through GradeScope (you will receive an invitation for CS145 Spring 2023).\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW1 conda environment using `cs145hw1.yml`. This file provides the env name and necessary packages for this tasks. If you have `conda` installed, you may create, activate and deactivate an environment using the following commands:\n",
    "\n",
    "```\n",
    "conda create -f cs145hw1.yml\n",
    "conda activate hw1\n",
    "conda deactivate\n",
    "```\n",
    "Here are some references about conda: [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You should not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you will need to highlight these changes. You may add additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression \n",
    "This example will walk you through three optimization algorithms for linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 100)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (1000, 100) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Closed form solution\n",
    "In this section, complete the `getBeta` function in `linear_regression.py`, which compute the close form solution of $\\hat{\\beta}$.\n",
    "\n",
    "To train you modelm use `lm.train('0')` function.\n",
    "\n",
    "Compute the training error and the testing error using `lm.predict` and `lm.compute_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  0\n",
      "Training error is:  0.08693886675396784\n",
      "Testing error is:  0.11017540281675801\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "## hint, for training error, you should get something around 0.08\n",
    "\n",
    "beta = lm.train('0')\n",
    "training_error = lm.compute_mse(lm.predict(lm.train_x, beta), lm.train_y)\n",
    "testing_error = lm.compute_mse(lm.predict(lm.test_x, beta), lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.Batch gradient descent\n",
    "In this section, complete the `getBetaBatchGradient` function in `linear_regression.py`, which computes the gradient of the objective fuction.\n",
    "\n",
    "To train you model, use `lm.train('1')` function.\n",
    "\n",
    "Compute the training error and the testing error using `lm.predict` and `lm.compute_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  1\n",
      "Training error is:  0.08693886675396784\n",
      "Testing error is:  0.11017540281675801\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "lm.train('1')\n",
    "training_error = lm.compute_mse(lm.predict(lm.train_x, beta), lm.train_y)\n",
    "testing_error = lm.compute_mse(lm.predict(lm.test_x, beta), lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.Stochastic gadient descent \n",
    "In this section, complete the `getBetaStochasticGradient` function in `linear_regression.py`, which computes an estimated gradient of the objective function.\n",
    "\n",
    "To train you model, use `lm.train('2')` function.\n",
    "\n",
    "Compute the training error and the testing error using `lm.predict` and `lm.compute_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error is:  0\n",
      "Testing error is:  0\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "lm.train('2')\n",
    "training_error = lm.compute_mse(lm.predict(lm.train_x, beta), lm.train_y)\n",
    "testing_error = lm.compute_mse(lm.predict(lm.test_x, beta), lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Ridge regression adds an L2 regularization term to the original objective function. The objective function becomes the following: \n",
    "    $$ J(\\beta) = \\frac{1}{2n} ||X\\beta - Y ||^2 + \\frac{\\lambda}{2n} \\beta^T\\beta ,$$ \n",
    "where $\\lambda \\leq 0$ is a hyper parameter that controls the trade-off. Take the derivative of this provided objective function and derive the new closed form solution for $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please type your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic regression \n",
    "This example will walk you through algorithms for logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 5)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "# As a sanity chech, we print out the size of the training data (1000, 5) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Batch gradiend descent\n",
    "In this section, complete the `getBeta_BatchGradient` in `logistic_regression.py`, which computes the gradient of the log likelihoood function. \n",
    "\n",
    "Complete the `compute_avglogL` function in `logistic_regression.py` for sanity check, you should get something around 0.46.\n",
    "\n",
    "To train you model, use `lm.train('0')` function.\n",
    "\n",
    "Compute the training and testing accuracy using `lm.predict` and `lm.compute_accuracy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is:  0\n",
      "Testing accuracy is:  0\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "lm.normalize()  # apply z-score normalization to the data\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Newton Raphhson\n",
    "In this section, complete the `getBeta_Newton` function in `logistic_regression.py`, which makes use of both first and second derivatives.\n",
    "\n",
    "To train you model, use `lm.train('1')` function.\n",
    "\n",
    "Compute the training and testing accuracy using `lm.predict` and `lm.compute_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is:  0\n",
      "Testing accuracy is:  0\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the accuracy on the testing dataset for each version. Are they the same? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please type your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Visualize the decision boundary on a toy dataset\n",
    "\n",
    "In this subsection, you will use the above implementation for another small dataset where each datapoint $x$ only has only two features $(x_1, x_2)$ to visualize the decision boundary of logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (99, 2)\n",
      "Training labels shape: (99,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression(verbose = False)\n",
    "lm.load_data('./data/logistic-regression-toy.csv','./data/logistic-regression-toy.csv')\n",
    "# As a sanity chech, we print out the size of the training data (99,2) and training labels (99,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, you can apply the same implementation of logistic regression model (either in 2.1 or 2.2) to the toy dataset. Print out the $\\hat{\\beta}$ after training and accuracy on the train set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is:  0\n"
     ]
    }
   ],
   "source": [
    "training_accuracy= 0\n",
    "lm.normalize()\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try to plot the decision boundary of your learned logistic regression classifier. Generally, a decision boundary is the region of a space in which the output label of a classifier is ambiguous. That is, in the given toy data, given a datapoint $x=(x_1, x_2)$ on the decision boundary, the logistic regression classifier cannot decide whether $y=0$ or $y=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Is the decision boundary for logistic regression linear? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Please type your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the decision boundary in the following cell. Note that the code to plot the raw data points are given. You may need `plt.plot` function (see [here](https://matplotlib.org/tutorials/introductory/pyplot.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot the raw data\n",
    "df = pd.concat([lm.train_x, lm.train_y], axis=1)\n",
    "groups = df.groupby(\"y\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[\"x1\"], group[\"x2\"], marker=\"o\", linestyle=\"\", label=name)\n",
    "\n",
    "# plot the decision boundary on top of the scattered points\n",
    "x1_vec = np.linspace(lm.train_x[\"x1\"].min(),lm.train_x[\"x1\"].max(),2)  ## x axis of the boundary is also given\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
