{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 3, Part 2: Neural Networks\n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW3 is due on **11:59 PM PT, May 15 (Monday)**. Please submit through GradeScope.\n",
    "\n",
    "Note that Howework #3 includes two jupyter notebooks (Part 1: kNN and Part 2: Neural Network), please merge the reports into one pdf in your submission.\n",
    "\n",
    "----\n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: XXX, UID: XXX** </span>\n",
    "\n",
    "----\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW3 conda environment specified in `cs145hw3.yml` file. If you have `conda` properly installed, you may create, activate or deactivate the env with the following commands:\n",
    "\n",
    "```\n",
    "conda env create -f cs145hw3.yml\n",
    "conda activate hw3\n",
    "conda deactivate\n",
    "```\n",
    "OR\n",
    "\n",
    "```\n",
    "conda env create --name NAMEOFYOURCHOICE -f cs145hw3.yml \n",
    "conda activate NAMEOFYOURCHOICE\n",
    "conda deactivate\n",
    "```\n",
    "To view the list of your environments, use the following command:\n",
    "```\n",
    "conda env list\n",
    "```\n",
    "\n",
    "References can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You must not delete any code cells in this notebook. If you change any code outside the blocks (such as hyperparameters) that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Backpropagation in a Neural Network\n",
    "\n",
    "<span style=\"color:red\"> Note: Section 1 is \"question-answer\" style problem. You do not need to code anything and you are required to calculate by hand (with a scientific calculator), which helps you understand the back propagation in neural networks. </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, let's consider a two-layer neural network and manually perform the forward and backward pass.\n",
    "For simplicity, we assume the input data is two dimensional.\n",
    "The model architecture is visualized in the figure below.\n",
    "Note that here we include the term `b` explicitly for each layer in the diagram.\n",
    "Recall the formula for computing $\\mathbf{x^{(l)}}$ in the l-th layer from $\\mathbf{x^{(l-1)}}$ in the (l-1)-th layer is $\\mathbf{x^{(l)}} = \\mathbf{f^{(l)}(W^{(l)} x^{(l-1)} + b^{(l)})}$.\n",
    "We use is the `sigmoid` as the activation function, i.e. $\\mathbf{f^{(l)}}(z) = \\frac{1}{1+\\exp(-z)}$.\n",
    "We use MSE as the loss function: $l\\mathbf{(y, \\hat y)} = \\frac{1}{2} ||\\mathbf{y - \\hat y}||^2$, where $\\hat y$ is the output ($\\mathbf{x}^{(2)}$) in our example.\n",
    "\n",
    "<img src=\"nn.png\"  width=\"350\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our weights as\n",
    "$$\\mathbf{W^{(1)}} = \\begin{bmatrix}\n",
    "0.15 & 0.2 \\\\\n",
    "0.25 & 0.3 \n",
    "\\end{bmatrix} \\in \\mathbb{R}^{d^{(1)}\\times d^{(0)}},\n",
    "\\quad \\mathbf{W^{(2)}} = [0.4, 0.45],\n",
    "\\quad \\mathbf{b^{(1)}} = \\begin{bmatrix}0.35 \\\\ 0.35\\end{bmatrix},\n",
    "\\quad \\mathbf{b^{(2)}} = 0.6$$\n",
    "\n",
    "TODO: specify the dimensionality of those vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "**Questions**\n",
    "\n",
    "1. For input $\\mathbf{x^{(0)}} = \\begin{bmatrix}0.05 \\\\ 0.1\\end{bmatrix}$, compute $\\mathbf{x^{(1)}}$ (Show your work).\n",
    "\n",
    "2. From $\\mathbf{x^{(1)}}$, compute $\\mathbf{x^{(2)}}$ in the output layer (Show your work).\n",
    "   \n",
    "3. If the target value $y = 0.01$, compute the MSE loss from $\\mathbf{x^{(2)}}$ (Show your work)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "After the forward process, we can perform a backward pass to compute the gradient of the weights in the neural network.\n",
    "\n",
    "**Questions**\n",
    "1. Consider the loss $l$ over the same input $\\mathbf{x^{(0)}} = \\begin{bmatrix}0.05 \\\\ 0.1\\end{bmatrix}$, calculate the gradient of $\\mathbf{W^{(2)}}$ and $\\mathbf{b^{(2)}}$, i.e. $\\frac{\\partial l}{\\partial \\mathbf{W^{(2)}}}$, $\\frac{\\partial l}{\\partial \\mathbf{b^{(2)}}}$  (Show your work).\n",
    "   \n",
    "2. Based on the result you computed in part 1, what will be the gradient of $\\mathbf{W^{(1)}}$ and $\\mathbf{b^{(1)}}$, i.e. $\\frac{\\partial l}{\\partial \\mathbf{W^{(1)}}}$, $\\frac{\\partial l}{\\partial \\mathbf{b^{(1)}}}$  (Show your work)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the analytical results\n",
    "\n",
    "You may run the code below to validate your solution above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(2, 2, bias=True)\n",
    "        self.l2 = nn.Linear(2, 1, bias=True)\n",
    "        \n",
    "        self.l1.weight.data = torch.Tensor([[0.15, 0.2], [0.25, 0.3]])\n",
    "        self.l2.weight.data = torch.Tensor([[0.4, 0.45]])\n",
    "        self.l1.bias.data = torch.Tensor([0.35, 0.35])\n",
    "        self.l2.bias.data = torch.Tensor([0.6])\n",
    "        \n",
    "    \n",
    "    def forward(self, x0):\n",
    "        z1 = self.l1(x0)\n",
    "        x1 = torch.sigmoid(z1)\n",
    "        z2 = self.l2(x1)\n",
    "        x2 = torch.sigmoid(z2)\n",
    "        print(\"z1\", z1)\n",
    "        print(\"x1\", x1)\n",
    "        print(\"z2\", z2)\n",
    "        print(\"x2\", x2)\n",
    "        \n",
    "        return x2\n",
    "\n",
    "    def loss(self, x2, y):\n",
    "        l = nn.MSELoss()\n",
    "        return 0.5 * l(x2, y)\n",
    "        \n",
    "x = torch.Tensor([0.05, 0.1])\n",
    "y = torch.Tensor([0.01])\n",
    "net = Net()\n",
    "y_hat = net(x)\n",
    "loss = net.loss(y_hat, y)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "print('-'*80)\n",
    "print(\"W1\", list(net.l1.parameters())[0].grad.numpy())\n",
    "print(\"b1\", list(net.l1.parameters())[1].grad.numpy())\n",
    "print(\"W2\", list(net.l2.parameters())[0].grad.numpy())\n",
    "print(\"b2\", list(net.l2.parameters())[1].grad.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Implementing a two-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define relative error function, which is used to check results later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example\n",
    "\n",
    "Before loading CIFAR-10, there will be a toy example to test your implementation of the forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3code.neural_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute forward pass scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement the forward pass of the neural network.\n",
    "\n",
    "# Note, there is a statement if y is None: return scores, which is why \n",
    "# the following call will calculate the scores.\n",
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "    [-1.07260209,  0.05083871, -0.87253915],\n",
    "    [-2.02778743, -0.10832494, -1.52641362],\n",
    "    [-0.74225908,  0.15259725, -0.39578548],\n",
    "    [-0.38172726,  0.10835902, -0.17328274],\n",
    "    [-0.64417314, -0.18886813, -0.41106892]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass loss\n",
    "\n",
    "The total loss includes data loss (MSE) and regularization loss, which is,\n",
    "\n",
    "$$L = L_{data}+L_{reg} = \\frac{1}{2N}\\sum_{i=1}^{N}\\left(\\boldsymbol{y}_{\\text{pred}}-\\boldsymbol{y}_{\\text{target}}\\right)^2 + \\frac{\\lambda}{2} \\left(||W_1||^2 + ||W_2||^2 \\right)$$\n",
    "\n",
    "More specifically in multi-class situation, if the output of neural nets from one sample is $y_{\\text{pred}}=(0.1,0.1,0.8)$ and $y_{\\text{target}}=(0,0,1)$ from the given label, then the MSE error will be $Error=(0.1-0)^2+(0.1-0)^2+(0.8-1)^2=0.06$\n",
    "\n",
    "Implement data loss and regularization loss. In the MSE function, you also need to return the gradients which need to be passed backward. This is similar to batch gradient in linear regression. Test your implementation of loss functions. The Difference should be less than 1e-12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "correct_loss_MSE = 1.8973332763705641 # check this number\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss_MSE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass (You do not need to implemented this part)\n",
    "\n",
    "We have already implemented the backwards pass of the neural network for you.  Run the block of code to check your gradients with the gradient check utilities provided. The results should be automatically correct (tiny relative error).\n",
    "\n",
    "If there is a gradient error larger than 1e-8, the training for neural networks later will be negatively affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('{} max relative error: {}'.format(param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Train the network via stochastic gradient descent, much like the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify CIFAR-10\n",
    "\n",
    "Do classification on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = './data/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SGD\n",
    "\n",
    "If your implementation is correct, you should see a validation accuracy of around 15-18%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-5, learning_rate_decay=0.95,\n",
    "            reg=0.1, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n",
    "\n",
    "# Save this net as the variable subopt_net for later comparison.\n",
    "subopt_net = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats['train_acc_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "The training accuracy isn't great. It seems even worse than simple KNN model, which is not as good as expected.\n",
    "\n",
    "(1) Identify some potential reasons for this result?\n",
    "\n",
    "(2) How should you fix the problems you identified in (1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the neural network\n",
    "\n",
    "Use the following part of the Jupyter notebook to optimize your hyperparameters on the validation set.  Store your nets in `best_net` variable. To get the full credit of the neural nets, you should get at least **45%** accuracy on validation set. \n",
    "\n",
    "**Reminder: Think about whether you should retrain a new model from scratch every time your try a new set of hyperparameters.** \\\n",
    "**Hint: potential hyperparameters include batch size, learning rate, regularization.** \\\n",
    "**Hint: You can achieve > 45% without changing the number of iterations (i.e. 1000).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net = None # store the best model into this \n",
    "\n",
    "input_size = 32 * 32 * 3 # do not change\n",
    "hidden_size = 50 # do not change\n",
    "num_classes = 10 # do not change\n",
    "best_valacc = 0 # record and update your best model validation accuracy\n",
    "best_net = None # store your best network\n",
    "\n",
    "# ================================================================ #\n",
    "# START YOUR CODE HERE:\n",
    "# ================================================================ #\n",
    "#   Optimize over your hyperparameters to arrive at the best neural\n",
    "#   network.  You should be able to get over 45% validation accuracy.\n",
    "#\n",
    "#   Note, you need to use the same network structure (keep hidden_size = 50)!\n",
    "# ================================================================ #\n",
    "\n",
    "# todo: optimal parameter search (you can use grid search with for-loops\n",
    "\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #\n",
    "# Output your results\n",
    "print(\"== Best parameter settings ==\")\n",
    "# print your best parameter setting here!\n",
    "print(\"Best accuracy on validation set: {}\".format(best_valacc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quesions**\n",
    "\n",
    "(1) What is your best parameter settings? (Output from the previous cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the weights of your neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.vis_utils import visualize_grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.T.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(subopt_net)\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "What's the difference between the weights in the suboptimal net and the best net? What did the weights in neural networks learn after training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "<span style=\"color:blue\"> Please write down your answer here! </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "#test_acc = (subopt_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## End of Homework 3, Part 2\n",
    "\n",
    "After you've finished both parts the homework, please export your ipynbs from two parts into two PDFs, and merge them into a single PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. Do not include any dataset in your submission.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
